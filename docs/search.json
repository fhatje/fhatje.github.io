[
  {
    "objectID": "posts/glomseg/train_model.html",
    "href": "posts/glomseg/train_model.html",
    "title": "Glomerulus Segmentation",
    "section": "",
    "text": "Training an Unet++ with fastai to segment glomeruli."
  },
  {
    "objectID": "posts/glomseg/train_model.html#goal",
    "href": "posts/glomseg/train_model.html#goal",
    "title": "Glomerulus Segmentation",
    "section": "Goal",
    "text": "Goal\nSegmenting glomeruli (an intricate structure in the kidney’s cortex in which the blood filtration happen), i.e. turning this\n to this"
  },
  {
    "objectID": "posts/glomseg/train_model.html#data",
    "href": "posts/glomseg/train_model.html#data",
    "title": "Glomerulus Segmentation",
    "section": "Data",
    "text": "Data\nI am using training data from the HuBMAP Challenge hosted on kaggle and a few dozen images downloaded from the Human Protein Atlas I annotated myself. (If you’re interested in how to do this, here a blogpost I wrote)\n\nimport numpy as np\nimport pandas as pd\nimport monai\nfrom fastai.vision.all import *\nimport segmentation_models_pytorch as smp\n\n\nORGAN = \"kidney\"\nTRAIN_BATCH_SIZE = 8 # Reduce this, if you run out of cuda memory\nEFFECTIVE_BATCH_SIZE = 8 # This is the batch size that will be used for training\nIMAGE_SIZE = 512\nLR = 3e-4 \nEPOCHS = 60\nMODEL_NAME = f\"smp_{IMAGE_SIZE}_{ORGAN}_added_data\"\nDATA_PATH = Path(\"../data/\")\n\n\n# Reproducibility\nTESTSET_SEED = 93\nTRAIN_VAL_SEED = 43\n\n\ndf = pd.read_csv(DATA_PATH/\"train.csv\") # This is the training set from the competition\nfns = L([*get_image_files(DATA_PATH/\"test_images\"), *get_image_files(DATA_PATH/\"train_images\")]) # List of all competition images   \nfn_col = [] # This will be a column in the dataframe, containing the filenames\nfor _, r in df.iterrows(): fn_col.append([fn for fn in fns if str(r[\"id\"]) == fn.stem][0])\ndf[\"fnames\"] = fn_col\ndf[\"is_organ\"] = df.organ.apply(lambda o: o==ORGAN)\ndf = df[df.is_organ] # Only keep images with the organ we are interested in\nassert df.organ.unique()[0] == ORGAN\ndf = df.drop(columns=\"organ data_source is_organ tissue_thickness pixel_size sex age\".split()).copy()\n\n\n# These are the images I added and that are annotated by me\nadd_images = get_image_files(DATA_PATH/\"add_images/\") \nadd_images_masks = get_image_files(DATA_PATH/\"segs/\")\n\n\n# The masks have the same name as the images, but with \"_mask\" appended\nmasks = [p.name[:-9]+\".png\" for p in add_images_masks] \n\n# Delete images without masks\nimages_to_delete = [p for p in add_images if p.name not in masks] \nfor p in images_to_delete: p.unlink()\n\n# This will contain the masks in the same order as the images\nsorted_masks = [] \nfor i in add_images:\n    sorted_masks.append([p for p in add_images_masks if i.stem == p.stem[:-5]][0])\n\n\n# Combine the competition data with the added data\nadd_df = pd.DataFrame({\n    \"fnames\": add_images,\n    \"segmentation\": sorted_masks,\n    \"is_add\": [True]*len(add_images)\n})\ndf[\"is_add\"] = df.id.apply(lambda p: False)\ncombined_df = pd.concat([df, add_df])\n\n\n# Setting aside a random testset\ncut = int(0.1 * len(combined_df))\nind = np.arange(len(combined_df))\nnp.random.seed(TESTSET_SEED) # Always create the same testset\nnp.random.shuffle(ind)\ntest_ind = ind[:cut]\ntrain_valid_ind = ind[cut:]\ntest_df = combined_df.iloc[test_ind,:].copy()\ntrain_df = combined_df.iloc[train_valid_ind,:].copy()\n\nThe masks of the competition data are in run-length encoding, that’s why we need the following function. It converts the run-length encoding to a numpy array which we can use for training.\n\n# From: https://www.kaggle.com/code/paulorzp/run-length-encode-and-decode/script\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return np.reshape(img, shape)\n\n\nCODES = [\"Background\", \"FTU\"] # FTU = functional tissue unit\n\n\ndef x_getter(r): return r[\"fnames\"]\ndef y_getter(r): \n    # My additional annotations are saved as pngs, so I need to differ between the two\n    if r[\"is_add\"]: \n        im = np.array(load_image(r[\"segmentation\"]), dtype=np.uint8)\n        im = (im.mean(axis=-1) < 125).astype(np.uint8)\n        return im\n    rle = r[\"rle\"]\n    shape = (int(r[\"img_height\"]), int(r[\"img_width\"]))\n    return rle_decode(rle, shape).T\n\n\nbtfms = aug_transforms(\n    mult=1.2,\n    do_flip=True,\n    flip_vert=True,\n    max_rotate=45.0,\n    min_zoom=1.,\n    max_zoom=1.5,\n    max_lighting=0.3,\n    max_warp=0.3,\n    size=(IMAGE_SIZE, IMAGE_SIZE),\n    p_affine=0.5\n) # Data augmentation\ndblock = DataBlock(blocks=(ImageBlock, MaskBlock(CODES)),\n                   get_x=x_getter,\n                   get_y=y_getter,\n                   splitter=RandomSplitter(seed=TRAIN_VAL_SEED),\n                   item_tfms=[Resize((IMAGE_SIZE, IMAGE_SIZE))],\n                   batch_tfms=btfms)\ndls = dblock.dataloaders(train_df, Path(\"..\"), bs=TRAIN_BATCH_SIZE)\n\n\ndls.train.show_batch()\n\n\n\n\n\ndls.valid.show_batch()\n\n\n\n\n\ncbs = [\n    GradientAccumulation(EFFECTIVE_BATCH_SIZE),\n    SaveModelCallback(fname=MODEL_NAME),\n]\n\n\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",        \n    encoder_weights=\"imagenet\",     \n    in_channels=3,                  \n    classes=2,                      \n)\n\n\n# Splitting model's into 2 groups to use fastai's differential learning rates\ndef splitter(model): \n    enc_params = L(model.encoder.parameters())\n    dec_params = L(model.decoder.parameters())\n    sg_params = L(model.segmentation_head.parameters())\n    untrained_params = L([*dec_params, *sg_params])\n    return L([enc_params, untrained_params])\n\n\nlearn = Learner(\n    dls, \n    model, \n    cbs=cbs,\n    splitter=splitter,\n    metrics=[Dice(), JaccardCoeff(), RocAucBinary()])\n\n\nlearn.fit_flat_cos(EPOCHS, LR)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      dice\n      jaccard_coeff\n      roc_auc_score\n      time\n    \n  \n  \n    \n      0\n      0.727570\n      0.603148\n      0.085859\n      0.044855\n      0.322773\n      00:12\n    \n    \n      1\n      0.604307\n      0.516570\n      0.096767\n      0.050843\n      0.427407\n      00:11\n    \n    \n      2\n      0.508117\n      0.379248\n      0.522587\n      0.353718\n      0.314975\n      00:11\n    \n    \n      3\n      0.424914\n      0.302621\n      0.584612\n      0.413040\n      0.085511\n      00:11\n    \n    \n      4\n      0.353784\n      0.207627\n      0.712757\n      0.553709\n      0.128719\n      00:11\n    \n    \n      5\n      0.296790\n      0.142182\n      0.830598\n      0.710276\n      0.556589\n      00:11\n    \n    \n      6\n      0.250124\n      0.107157\n      0.849849\n      0.738903\n      0.781829\n      00:11\n    \n    \n      7\n      0.211806\n      0.088498\n      0.857634\n      0.750752\n      0.867223\n      00:11\n    \n    \n      8\n      0.180824\n      0.070990\n      0.859145\n      0.753071\n      0.888895\n      00:11\n    \n    \n      9\n      0.154632\n      0.061644\n      0.856697\n      0.749318\n      0.929199\n      00:11\n    \n    \n      10\n      0.133044\n      0.052796\n      0.868353\n      0.767336\n      0.953055\n      00:11\n    \n    \n      11\n      0.115005\n      0.046287\n      0.871848\n      0.772811\n      0.964127\n      00:11\n    \n    \n      12\n      0.099967\n      0.040762\n      0.883804\n      0.791800\n      0.971401\n      00:11\n    \n    \n      13\n      0.087097\n      0.037157\n      0.889235\n      0.800560\n      0.976527\n      00:11\n    \n    \n      14\n      0.076725\n      0.033152\n      0.899692\n      0.817673\n      0.979948\n      00:12\n    \n    \n      15\n      0.068125\n      0.031111\n      0.902548\n      0.822404\n      0.982294\n      00:11\n    \n    \n      16\n      0.060929\n      0.027760\n      0.910408\n      0.835549\n      0.984009\n      00:11\n    \n    \n      17\n      0.054637\n      0.027974\n      0.894529\n      0.809184\n      0.983233\n      00:11\n    \n    \n      18\n      0.049165\n      0.024334\n      0.911780\n      0.837864\n      0.986022\n      00:11\n    \n    \n      19\n      0.044277\n      0.024281\n      0.905008\n      0.826498\n      0.983716\n      00:11\n    \n    \n      20\n      0.040233\n      0.022132\n      0.913338\n      0.840498\n      0.986142\n      00:11\n    \n    \n      21\n      0.036737\n      0.021514\n      0.913032\n      0.839981\n      0.982759\n      00:12\n    \n    \n      22\n      0.033632\n      0.020441\n      0.919236\n      0.850543\n      0.988929\n      00:11\n    \n    \n      23\n      0.031428\n      0.021564\n      0.900877\n      0.819632\n      0.976978\n      00:11\n    \n    \n      24\n      0.029265\n      0.018416\n      0.922131\n      0.855513\n      0.990599\n      00:11\n    \n    \n      25\n      0.027243\n      0.017509\n      0.924026\n      0.858781\n      0.990082\n      00:11\n    \n    \n      26\n      0.025496\n      0.018210\n      0.916627\n      0.846087\n      0.983598\n      00:11\n    \n    \n      27\n      0.024097\n      0.017901\n      0.917531\n      0.847628\n      0.987033\n      00:11\n    \n    \n      28\n      0.022979\n      0.016894\n      0.920493\n      0.852697\n      0.986602\n      00:11\n    \n    \n      29\n      0.021918\n      0.018380\n      0.916335\n      0.845588\n      0.989573\n      00:11\n    \n    \n      30\n      0.020899\n      0.016375\n      0.920465\n      0.852650\n      0.988758\n      00:11\n    \n    \n      31\n      0.019700\n      0.014662\n      0.929562\n      0.868394\n      0.988694\n      00:11\n    \n    \n      32\n      0.018992\n      0.018643\n      0.906843\n      0.829563\n      0.989722\n      00:11\n    \n    \n      33\n      0.018747\n      0.016715\n      0.918087\n      0.848578\n      0.986600\n      00:11\n    \n    \n      34\n      0.018041\n      0.015872\n      0.918797\n      0.849792\n      0.982712\n      00:11\n    \n    \n      35\n      0.017152\n      0.015625\n      0.922364\n      0.855914\n      0.979253\n      00:11\n    \n    \n      36\n      0.016359\n      0.014718\n      0.925542\n      0.861403\n      0.977279\n      00:11\n    \n    \n      37\n      0.015855\n      0.012931\n      0.932580\n      0.873676\n      0.987032\n      00:11\n    \n    \n      38\n      0.015509\n      0.012442\n      0.932764\n      0.873999\n      0.990109\n      00:11\n    \n    \n      39\n      0.015077\n      0.011627\n      0.936922\n      0.881329\n      0.989619\n      00:11\n    \n    \n      40\n      0.014742\n      0.011964\n      0.934999\n      0.877933\n      0.987285\n      00:11\n    \n    \n      41\n      0.014460\n      0.011784\n      0.937937\n      0.883128\n      0.990435\n      00:11\n    \n    \n      42\n      0.014327\n      0.013164\n      0.928118\n      0.865877\n      0.985723\n      00:11\n    \n    \n      43\n      0.014337\n      0.012003\n      0.934332\n      0.876757\n      0.987117\n      00:11\n    \n    \n      44\n      0.014002\n      0.012138\n      0.935304\n      0.878470\n      0.988768\n      00:11\n    \n    \n      45\n      0.013552\n      0.011391\n      0.936339\n      0.880298\n      0.987371\n      00:11\n    \n    \n      46\n      0.013122\n      0.012408\n      0.929143\n      0.867663\n      0.983431\n      00:11\n    \n    \n      47\n      0.012708\n      0.011765\n      0.932473\n      0.873489\n      0.990839\n      00:12\n    \n    \n      48\n      0.012281\n      0.010892\n      0.938805\n      0.884668\n      0.987427\n      00:11\n    \n    \n      49\n      0.011792\n      0.010427\n      0.941303\n      0.889115\n      0.986761\n      00:11\n    \n    \n      50\n      0.011579\n      0.011140\n      0.938671\n      0.884429\n      0.982990\n      00:11\n    \n    \n      51\n      0.011452\n      0.010431\n      0.945001\n      0.895737\n      0.985848\n      00:11\n    \n    \n      52\n      0.011334\n      0.010210\n      0.943139\n      0.892396\n      0.987449\n      00:11\n    \n    \n      53\n      0.011300\n      0.011088\n      0.937227\n      0.881870\n      0.987982\n      00:11\n    \n    \n      54\n      0.011329\n      0.010670\n      0.937806\n      0.882895\n      0.988177\n      00:11\n    \n    \n      55\n      0.011342\n      0.010610\n      0.937321\n      0.882035\n      0.989792\n      00:11\n    \n    \n      56\n      0.011146\n      0.010468\n      0.938884\n      0.884808\n      0.988132\n      00:11\n    \n    \n      57\n      0.011024\n      0.010291\n      0.940346\n      0.887408\n      0.988785\n      00:11\n    \n    \n      58\n      0.010618\n      0.010369\n      0.939683\n      0.886228\n      0.988485\n      00:11\n    \n    \n      59\n      0.010563\n      0.010375\n      0.939610\n      0.886099\n      0.988654\n      00:11\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.6031481027603149.\nBetter model found at epoch 1 with valid_loss value: 0.516569972038269.\nBetter model found at epoch 2 with valid_loss value: 0.3792479634284973.\nBetter model found at epoch 3 with valid_loss value: 0.302621066570282.\nBetter model found at epoch 4 with valid_loss value: 0.20762743055820465.\nBetter model found at epoch 5 with valid_loss value: 0.14218230545520782.\nBetter model found at epoch 6 with valid_loss value: 0.10715709626674652.\nBetter model found at epoch 7 with valid_loss value: 0.0884983241558075.\nBetter model found at epoch 8 with valid_loss value: 0.07099024951457977.\nBetter model found at epoch 9 with valid_loss value: 0.06164400279521942.\nBetter model found at epoch 10 with valid_loss value: 0.0527963824570179.\nBetter model found at epoch 11 with valid_loss value: 0.046286918222904205.\nBetter model found at epoch 12 with valid_loss value: 0.04076218977570534.\nBetter model found at epoch 13 with valid_loss value: 0.037156544625759125.\nBetter model found at epoch 14 with valid_loss value: 0.0331517718732357.\nBetter model found at epoch 15 with valid_loss value: 0.031111164018511772.\nBetter model found at epoch 16 with valid_loss value: 0.027760174125432968.\nBetter model found at epoch 18 with valid_loss value: 0.024333978071808815.\nBetter model found at epoch 19 with valid_loss value: 0.024281086400151253.\nBetter model found at epoch 20 with valid_loss value: 0.022132011130452156.\nBetter model found at epoch 21 with valid_loss value: 0.021513625979423523.\nBetter model found at epoch 22 with valid_loss value: 0.020441483706235886.\nBetter model found at epoch 24 with valid_loss value: 0.018416451290249825.\nBetter model found at epoch 25 with valid_loss value: 0.017508765682578087.\nBetter model found at epoch 28 with valid_loss value: 0.016893696039915085.\nBetter model found at epoch 30 with valid_loss value: 0.016375476494431496.\nBetter model found at epoch 31 with valid_loss value: 0.0146623644977808.\nBetter model found at epoch 37 with valid_loss value: 0.012930507771670818.\nBetter model found at epoch 38 with valid_loss value: 0.01244184747338295.\nBetter model found at epoch 39 with valid_loss value: 0.01162666454911232.\nBetter model found at epoch 45 with valid_loss value: 0.011391145177185535.\nBetter model found at epoch 48 with valid_loss value: 0.010892268270254135.\nBetter model found at epoch 49 with valid_loss value: 0.010426941327750683.\nBetter model found at epoch 52 with valid_loss value: 0.010209585539996624.\n\n\n\nlearn.load(MODEL_NAME)\n\nSaved filed doesn't contain an optimizer state.\n\n\n<fastai.learner.Learner at 0x7f2dd019eac0>\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a dataloader from the testset\ntest_dl = dls.test_dl(test_df, with_labels=True)\ndice_func = monai.metrics.DiceMetric(include_background=False, reduction=\"mean\")\n\n\n# This function steps through the different thresholds and returns the best one\ndef get_best_threshold(learn, dl, metric_func, n_steps=17):\n    \"\"\"\n    Tests `n_steps` different thresholds.\n    Return the best threshold and the corresonding score.\n    \"\"\"\n    thresholds = torch.linspace(0.1, 0.9, n_steps)\n    results = []\n\n    res = learn.get_preds(dl=dl, with_input=False, with_targs=True, act=partial(F.softmax, dim=1))\n    \n    for t in thresholds:\n        metric_func((res[0][:,1]>t).unsqueeze(1), res[-1].unsqueeze(1))\n        metric = metric_func.aggregate().item()\n        metric_func.reset()\n        results.append((round(t.detach().cpu().item(), ndigits=3), metric))\n\n    return sorted(results, key=lambda tpl: tpl[1], reverse=True)[0]\n\n\nbest_threshold, _ = get_best_threshold(learn, dls.valid, dice_func)\nbest_threshold\n\n\n\n\n\n\n\n\n0.6\n\n\nAnd now we test the model on the training, validation and test set with the best threshold.\n\ndef test_model(learn, dl, metric_func, threshold=0.5):\n    res = learn.get_preds(dl=dl, with_input=False, with_targs=True, act=partial(F.softmax, dim=1))\n    metric_func((res[0][:,1]>threshold).unsqueeze(1), res[-1].unsqueeze(1))\n    metric = metric_func.aggregate().item()\n    metric_func.reset()\n    return metric\n    \ntrain_dice = test_model(learn, dls.train, dice_func, threshold=best_threshold)\nvalid_dice = test_model(learn, dls.valid, dice_func, threshold=best_threshold)\ntest_dice  = test_model(learn, test_dl,   dice_func, threshold=best_threshold)\n\nfor s, d in zip((\"Training Dice:\", \"Valid Dice\", \"Test Dice\"), (train_dice, valid_dice, test_dice)):\n    print(s, d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Dice: 0.9252251982688904\nValid Dice 0.9271063208580017\nTest Dice 0.9414731860160828\n\n\n\n# Save and export the model\nBEST_MODEL_NAME = f\"unetpp_b4_th{int(best_threshold*100)}_d{str(test_dice)[2:6]}\"\nlearn.save(BEST_MODEL_NAME)\nlearn.export(BEST_MODEL_NAME+\".pkl\")\n\nA live version of this model is deployed on a huggingface space."
  },
  {
    "objectID": "posts/hpa/protein_atlas_scrape.html",
    "href": "posts/hpa/protein_atlas_scrape.html",
    "title": "Scraping Images from the Human Protein Atlas",
    "section": "",
    "text": "We will be scraping histologic tissue slices from the Human Protein Atlas, which they explicitly allow here.\nThey provide datasets of available images and respective metadata. I am only interested in largeintestine, kidney, lung, spleen, prostate, preferably without stains.\nHere is an (cropped) example:\n\n\nimport numpy as np\nimport pandas as pd\nimport shutil\nfrom bs4 import BeautifulSoup as BS\nimport time\nfrom tqdm import tqdm\nimport requests\nfrom pathlib import Path\nfrom PIL import Image\n\n\ndf = pd.read_csv(\"https://www.proteinatlas.org/download/normal_tissue.tsv.zip\", delimiter=\"\\t\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gene\n      Gene name\n      Tissue\n      Cell type\n      Level\n      Reliability\n    \n  \n  \n    \n      0\n      ENSG00000000003\n      TSPAN6\n      adipose tissue\n      adipocytes\n      Not detected\n      Approved\n    \n    \n      1\n      ENSG00000000003\n      TSPAN6\n      adrenal gland\n      glandular cells\n      Not detected\n      Approved\n    \n    \n      2\n      ENSG00000000003\n      TSPAN6\n      appendix\n      glandular cells\n      Medium\n      Approved\n    \n    \n      3\n      ENSG00000000003\n      TSPAN6\n      appendix\n      lymphoid tissue\n      Not detected\n      Approved\n    \n    \n      4\n      ENSG00000000003\n      TSPAN6\n      bone marrow\n      hematopoietic cells\n      Not detected\n      Approved\n    \n  \n\n\n\n\nLet’s see which tissues they have images of.\n\ndf.Tissue.unique()\n\narray(['adipose tissue', 'adrenal gland', 'appendix', 'bone marrow',\n       'breast', 'bronchus', 'caudate', 'cerebellum', 'cerebral cortex',\n       'cervix', 'colon', 'duodenum', 'endometrium 1', 'endometrium 2',\n       'epididymis', 'esophagus', 'fallopian tube', 'gallbladder',\n       'heart muscle', 'hippocampus', 'kidney', 'liver', 'lung',\n       'lymph node', 'nasopharynx', 'oral mucosa', 'ovary', 'pancreas',\n       'parathyroid gland', 'placenta', 'prostate', 'rectum',\n       'salivary gland', 'seminal vesicle', 'skeletal muscle', 'skin 1',\n       'skin 2', 'small intestine', 'smooth muscle', 'soft tissue 1',\n       'soft tissue 2', 'spleen', 'stomach 1', 'stomach 2', 'testis',\n       'thyroid gland', 'tonsil', 'urinary bladder', 'vagina', nan,\n       'hypothalamus', 'endometrium', 'hair', 'retina',\n       'lactating breast', 'skin', 'thymus', 'cartilage', 'eye',\n       'pituitary gland', 'choroid plexus', 'dorsal raphe',\n       'substantia nigra', 'sole of foot'], dtype=object)\n\n\nEvery tissue I am interest in is present! (Colon = large Intestine) Let’s get rid of everything else.\n\ntissues = set(\"colon kidney lung prostate spleen\".split())\nmask = df.Tissue.isin(tissues)\ndf = df[mask]\ndf.Tissue.unique()\n\narray(['colon', 'kidney', 'lung', 'prostate', 'spleen'], dtype=object)\n\n\nAll images have been stained with some antibody, but we actually don’t want stained images. That’s why we’re only keeping those images in which no antigen has been detected (meaning no visible stain is present in these images).\n\ndf = df[df.Level == \"Not detected\"]\ndf.Level.unique()\n\narray(['Not detected'], dtype=object)\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gene\n      Gene name\n      Tissue\n      Cell type\n      Level\n      Reliability\n    \n  \n  \n    \n      20\n      ENSG00000000003\n      TSPAN6\n      colon\n      endothelial cells\n      Not detected\n      Approved\n    \n    \n      22\n      ENSG00000000003\n      TSPAN6\n      colon\n      peripheral nerve/ganglion\n      Not detected\n      Approved\n    \n    \n      35\n      ENSG00000000003\n      TSPAN6\n      kidney\n      cells in glomeruli\n      Not detected\n      Approved\n    \n    \n      40\n      ENSG00000000003\n      TSPAN6\n      lung\n      macrophages\n      Not detected\n      Approved\n    \n    \n      67\n      ENSG00000000003\n      TSPAN6\n      spleen\n      cells in red pulp\n      Not detected\n      Approved\n    \n  \n\n\n\n\nCell types and reliability are not relevant in this usecase, so let’s just get rid of those columns. Also, the actual name of the gene is redundant.\n\ndel df[\"Cell type\"], df[\"Reliability\"], df[\"Level\"], df[\"Gene name\"]\ndf\n\n\n\n\n\n  \n    \n      \n      Gene\n      Tissue\n    \n  \n  \n    \n      20\n      ENSG00000000003\n      colon\n    \n    \n      22\n      ENSG00000000003\n      colon\n    \n    \n      35\n      ENSG00000000003\n      kidney\n    \n    \n      40\n      ENSG00000000003\n      lung\n    \n    \n      67\n      ENSG00000000003\n      spleen\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1194410\n      ENSG00000288602\n      kidney\n    \n    \n      1194414\n      ENSG00000288602\n      lung\n    \n    \n      1194415\n      ENSG00000288602\n      lung\n    \n    \n      1194416\n      ENSG00000288602\n      lung\n    \n    \n      1194457\n      ENSG00000288602\n      spleen\n    \n  \n\n67054 rows × 2 columns\n\n\n\nSome genes appear multiple times per organ. Let’s remove those duplicates.\n\ndf = df.drop_duplicates(subset=[\"Gene\", \"Tissue\"])\ndf\n\n\n\n\n\n  \n    \n      \n      Gene\n      Tissue\n    \n  \n  \n    \n      20\n      ENSG00000000003\n      colon\n    \n    \n      35\n      ENSG00000000003\n      kidney\n    \n    \n      40\n      ENSG00000000003\n      lung\n    \n    \n      67\n      ENSG00000000003\n      spleen\n    \n    \n      196\n      ENSG00000000457\n      lung\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1194296\n      ENSG00000288558\n      lung\n    \n    \n      1194388\n      ENSG00000288602\n      colon\n    \n    \n      1194410\n      ENSG00000288602\n      kidney\n    \n    \n      1194414\n      ENSG00000288602\n      lung\n    \n    \n      1194457\n      ENSG00000288602\n      spleen\n    \n  \n\n37915 rows × 2 columns\n\n\n\nNow, how many genes per organ do we get?\n\ndf.groupby(\"Tissue\").count()\n\n\n\n\n\n  \n    \n      \n      Gene\n    \n    \n      Tissue\n      \n    \n  \n  \n    \n      colon\n      7338\n    \n    \n      kidney\n      7732\n    \n    \n      lung\n      8747\n    \n    \n      prostate\n      5174\n    \n    \n      spleen\n      8924\n    \n  \n\n\n\n\nTo many to just download all at once, so we will randomly select 25 of each tissue. Since many genes have been more than one image, we will and up which 2-3 times as many images.\n\nrand_indices = []\nfor t in df.Tissue.unique():\n    rand_idx = list(np.random.choice(df[df.Tissue==t].index, size=25))\n    rand_indices = [*rand_indices, *rand_idx]\ndf = df.loc[rand_indices]\ndf.groupby(\"Tissue\").count()\n\n\n\n\n\n  \n    \n      \n      Gene\n    \n    \n      Tissue\n      \n    \n  \n  \n    \n      colon\n      25\n    \n    \n      kidney\n      25\n    \n    \n      lung\n      25\n    \n    \n      prostate\n      25\n    \n    \n      spleen\n      25\n    \n  \n\n\n\n\nNow, we need to build the URL’s of those images. Most genes have multiple images so we are going to expand our dataset with links to every image of every gene.\n\ndef build_url(embl, tissue): return f\"https://www.proteinatlas.org/{embl}/tissue/{tissue}\"\ndef extract_image_urls(htext):\n    soup = BS(htext)\n    image_urls = soup.body.findAll(\"img\")\n    image_urls = [il.get_attribute_list(\"src\")[0] for il in image_urls]\n    image_urls = [il for il in image_urls if \"proteinatlas\" in il]\n    image_urls = [\"https:\"+im_url for im_url in image_urls]\n    return image_urls\n\n\nurls = []\nfor gene, tissue in tqdm(list(zip(df.Gene.values, df.Tissue.values))):\n    url = build_url(gene, tissue)\n    r = requests.get(url)\n    if r.status_code != 200:\n        print(f\"Url: {url} didn't work\")\n        urls.append(None)\n        continue\n    urls.append(\";\".join(extract_image_urls(r.text)))\n    time.sleep(0.1)\ndf[\"urls\"] = urls\n\n100%|██████████| 125/125 [01:39<00:00,  1.25it/s]\n\n\nWe save the data, so that we don’t have to run this code twice.\n\ndf.to_csv(\"protein_atlas_scrape.csv\", index=False)\n\nAnd finally, we download the images.\n\ndef download_image(image_url, filename):\n    try:\n        r = requests.get(image_url, stream=True)\n        if r.status_code == 200:\n            r.raw.decode_content = True\n            with open(filename,'wb') as f: shutil.copyfileobj(r.raw, f)\n        else: return\n    except: return\n\n\nDOWNLOAD_DIR = Path(\"images\")\nDOWNLOAD_DIR.mkdir(exist_ok=True)\nfor _,row in tqdm(df.iterrows(), total=len(df)):\n    for i, url in enumerate(row.urls.split(\";\")):\n        image_name = DOWNLOAD_DIR/f\"{row.Gene}_{row.Tissue}_{i}.jpg\"\n        download_image(url, image_name)\n        time.sleep(0.1)\n\n100%|██████████| 125/125 [03:17<00:00,  1.58s/it]\n\n\n\nlen(list(Path(\"images\").iterdir()))\n\n506\n\n\nNow we have 506 images! Let’s look at a random one.\n\nimage_file = np.random.choice(list(Path(\"images\").iterdir()))\nImage.open(image_file)\n\n\n\n\nRandom Image from our new new dataset.\n\n\n\n\nAnd there we go! We now have histologic images we can use for anything we like."
  },
  {
    "objectID": "blog_overview.html",
    "href": "blog_overview.html",
    "title": "Blog",
    "section": "",
    "text": "Glomerulus Segmentation\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\nFavian Hatje\n\n\n\n\n\n\n  \n\n\n\n\nScraping Images from the Human Protein Atlas\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2022\n\n\nFavian Hatje\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "old_index.html",
    "href": "old_index.html",
    "title": "Welcome",
    "section": "",
    "text": "Favian Hatje is a radiologist with a passion for Science, especially data science and artificial intelligence, currently working for Ihre-Radiologen in Berlin. He studied Medicine in Hamburg where he also worked fulltime in a physiology lab for his medical doctorate and, when not busy reading CT or MRI studies, spends most of his time competing on Kaggle.\n\n\n\n\n\n\nMedicine\nArtificial Intelligence\n(Data) Science\n\n\n\n\n\nMedical Studies in Hamburg, Germany 2012 - 2019\nMedical Doctorate 2017 - 2022\n\n\n\n\n\nHatje FA, Wedekind U, Sachs W, Loreth D, Reichelt J, Demir F, Kosub C, Heintz L, Tomas NM, Huber TB, Skuza S, Sachs M, Zielinski S, Rinschen MM, Meyer-Schwesinger C. Tripartite Separation of Glomerular Cell Types and Proteomes from Reporter-Free Mice. J Am Soc Nephrol. 2021 Sep;32(9):2175-2193. doi: 10.1681/ASN.2020091346. Epub 2021 Jun 1. PMID: 34074698; PMCID: PMC8729851."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am a radiologist with a passion for science and technology. I have many interests, but I am particularly drawn to the field of artificial intelligence. I am based in Berlin, Germany, where I work for Ihre-Radiologen.\nMy journey began in Hamburg, where I studied medicine, complemented by one year of full-time work in a physiology lab. There, my colleagues and I developed a method for extracting large quantities of glomerular cells from mice, which resulted in a publication in the Journal of the American Society of Nephrology (JASN), a renowned journal in the field of nephrology.\nI then made my way to Berlin, where I began my career as a radiologist. By day, I am busy reading CT and MRI studies, at night I enjoy exploring the limitless possibilities and incredible developments of artificial intelligence. Through continued self-study through courses like fast.ai and Deep Learning for Medicine on Coursera, I have deepened my understanding of this exciting field, and I regularly participate in Kaggle competitions to hone my skills.\nIf you wish to connect, feel free to reach out on Twitter or via email at favian.hatje@outlook.com. Otherwise I invite you to have a look at my blog, where I share notebooks documenting what I do and learn.\n\nEducation:\n\nMedical Studies in Hamburg, Germany 2012 - 2019\nMedical Doctorate 2017 - 2023\n\n\n\nPublications:\n\nHatje FA, Wedekind U, Sachs W, Loreth D, Reichelt J, Demir F, Kosub C, Heintz L, Tomas NM, Huber TB, Skuza S, Sachs M, Zielinski S, Rinschen MM, Meyer-Schwesinger C. Tripartite Separation of Glomerular Cell Types and Proteomes from Reporter-Free Mice. J Am Soc Nephrol. 2021 Sep;32(9):2175-2193. doi: 10.1681/ASN.2020091346. Epub 2021 Jun 1. PMID: 34074698; PMCID: PMC8729851."
  }
]