[
  {
    "objectID": "posts/glomseg/train_model.html",
    "href": "posts/glomseg/train_model.html",
    "title": "Glomerulus Segmentation",
    "section": "",
    "text": "Training an Unet++ with fastai to segment glomeruli."
  },
  {
    "objectID": "posts/glomseg/train_model.html#goal",
    "href": "posts/glomseg/train_model.html#goal",
    "title": "Glomerulus Segmentation",
    "section": "Goal",
    "text": "Goal\nSegmenting glomeruli (an intricate structure in the kidney’s cortex in which the blood filtration happen), i.e. turning this\n to this"
  },
  {
    "objectID": "posts/glomseg/train_model.html#data",
    "href": "posts/glomseg/train_model.html#data",
    "title": "Glomerulus Segmentation",
    "section": "Data",
    "text": "Data\nI am using training data from the HuBMAP Challenge hosted on kaggle and a few dozen images downloaded from the Human Protein Atlas I annotated myself. (If you’re interested in how to do this, here a blogpost I wrote)\n\nimport numpy as np\nimport pandas as pd\nimport monai\nfrom fastai.vision.all import *\nimport segmentation_models_pytorch as smp\n\n\nORGAN = \"kidney\"\nTRAIN_BATCH_SIZE = 8 # Reduce this, if you run out of cuda memory\nEFFECTIVE_BATCH_SIZE = 8 # This is the batch size that will be used for training\nIMAGE_SIZE = 512\nLR = 3e-4 \nEPOCHS = 60\nMODEL_NAME = f\"smp_{IMAGE_SIZE}_{ORGAN}_added_data\"\nDATA_PATH = Path(\"../data/\")\n\n\n# Reproducibility\nTESTSET_SEED = 93\nTRAIN_VAL_SEED = 43\n\n\ndf = pd.read_csv(DATA_PATH/\"train.csv\") # This is the training set from the competition\nfns = L([*get_image_files(DATA_PATH/\"test_images\"), *get_image_files(DATA_PATH/\"train_images\")]) # List of all competition images   \nfn_col = [] # This will be a column in the dataframe, containing the filenames\nfor _, r in df.iterrows(): fn_col.append([fn for fn in fns if str(r[\"id\"]) == fn.stem][0])\ndf[\"fnames\"] = fn_col\ndf[\"is_organ\"] = df.organ.apply(lambda o: o==ORGAN)\ndf = df[df.is_organ] # Only keep images with the organ we are interested in\nassert df.organ.unique()[0] == ORGAN\ndf = df.drop(columns=\"organ data_source is_organ tissue_thickness pixel_size sex age\".split()).copy()\n\n\n# These are the images I added and that are annotated by me\nadd_images = get_image_files(DATA_PATH/\"add_images/\") \nadd_images_masks = get_image_files(DATA_PATH/\"segs/\")\n\n\n# The masks have the same name as the images, but with \"_mask\" appended\nmasks = [p.name[:-9]+\".png\" for p in add_images_masks] \n\n# Delete images without masks\nimages_to_delete = [p for p in add_images if p.name not in masks] \nfor p in images_to_delete: p.unlink()\n\n# This will contain the masks in the same order as the images\nsorted_masks = [] \nfor i in add_images:\n    sorted_masks.append([p for p in add_images_masks if i.stem == p.stem[:-5]][0])\n\n\n# Combine the competition data with the added data\nadd_df = pd.DataFrame({\n    \"fnames\": add_images,\n    \"segmentation\": sorted_masks,\n    \"is_add\": [True]*len(add_images)\n})\ndf[\"is_add\"] = df.id.apply(lambda p: False)\ncombined_df = pd.concat([df, add_df])\n\n\n# Setting aside a random testset\ncut = int(0.1 * len(combined_df))\nind = np.arange(len(combined_df))\nnp.random.seed(TESTSET_SEED) # Always create the same testset\nnp.random.shuffle(ind)\ntest_ind = ind[:cut]\ntrain_valid_ind = ind[cut:]\ntest_df = combined_df.iloc[test_ind,:].copy()\ntrain_df = combined_df.iloc[train_valid_ind,:].copy()\n\nThe masks of the competition data are in run-length encoding, that’s why we need the following function. It converts the run-length encoding to a numpy array which we can use for training.\n\n# From: https://www.kaggle.com/code/paulorzp/run-length-encode-and-decode/script\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return np.reshape(img, shape)\n\n\nCODES = [\"Background\", \"FTU\"] # FTU = functional tissue unit\n\n\ndef x_getter(r): return r[\"fnames\"]\ndef y_getter(r): \n    # My additional annotations are saved as pngs, so I need to differ between the two\n    if r[\"is_add\"]: \n        im = np.array(load_image(r[\"segmentation\"]), dtype=np.uint8)\n        im = (im.mean(axis=-1) < 125).astype(np.uint8)\n        return im\n    rle = r[\"rle\"]\n    shape = (int(r[\"img_height\"]), int(r[\"img_width\"]))\n    return rle_decode(rle, shape).T\n\n\nbtfms = aug_transforms(\n    mult=1.2,\n    do_flip=True,\n    flip_vert=True,\n    max_rotate=45.0,\n    min_zoom=1.,\n    max_zoom=1.5,\n    max_lighting=0.3,\n    max_warp=0.3,\n    size=(IMAGE_SIZE, IMAGE_SIZE),\n    p_affine=0.5\n) # Data augmentation\ndblock = DataBlock(blocks=(ImageBlock, MaskBlock(CODES)),\n                   get_x=x_getter,\n                   get_y=y_getter,\n                   splitter=RandomSplitter(seed=TRAIN_VAL_SEED),\n                   item_tfms=[Resize((IMAGE_SIZE, IMAGE_SIZE))],\n                   batch_tfms=btfms)\ndls = dblock.dataloaders(train_df, Path(\"..\"), bs=TRAIN_BATCH_SIZE)\n\n\ndls.train.show_batch()\n\n\n\n\n\ndls.valid.show_batch()\n\n\n\n\n\ncbs = [\n    GradientAccumulation(EFFECTIVE_BATCH_SIZE),\n    SaveModelCallback(fname=MODEL_NAME),\n]\n\n\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b4\",        \n    encoder_weights=\"imagenet\",     \n    in_channels=3,                  \n    classes=2,                      \n)\n\n\n# Splitting model's into 2 groups to use fastai's differential learning rates\ndef splitter(model): \n    enc_params = L(model.encoder.parameters())\n    dec_params = L(model.decoder.parameters())\n    sg_params = L(model.segmentation_head.parameters())\n    untrained_params = L([*dec_params, *sg_params])\n    return L([enc_params, untrained_params])\n\n\nlearn = Learner(\n    dls, \n    model, \n    cbs=cbs,\n    splitter=splitter,\n    metrics=[Dice(), JaccardCoeff(), RocAucBinary()])\n\n\nlearn.fit_flat_cos(EPOCHS, LR)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      dice\n      jaccard_coeff\n      roc_auc_score\n      time\n    \n  \n  \n    \n      0\n      0.727570\n      0.603148\n      0.085859\n      0.044855\n      0.322773\n      00:12\n    \n    \n      1\n      0.604307\n      0.516570\n      0.096767\n      0.050843\n      0.427407\n      00:11\n    \n    \n      2\n      0.508117\n      0.379248\n      0.522587\n      0.353718\n      0.314975\n      00:11\n    \n    \n      3\n      0.424914\n      0.302621\n      0.584612\n      0.413040\n      0.085511\n      00:11\n    \n    \n      4\n      0.353784\n      0.207627\n      0.712757\n      0.553709\n      0.128719\n      00:11\n    \n    \n      5\n      0.296790\n      0.142182\n      0.830598\n      0.710276\n      0.556589\n      00:11\n    \n    \n      6\n      0.250124\n      0.107157\n      0.849849\n      0.738903\n      0.781829\n      00:11\n    \n    \n      7\n      0.211806\n      0.088498\n      0.857634\n      0.750752\n      0.867223\n      00:11\n    \n    \n      8\n      0.180824\n      0.070990\n      0.859145\n      0.753071\n      0.888895\n      00:11\n    \n    \n      9\n      0.154632\n      0.061644\n      0.856697\n      0.749318\n      0.929199\n      00:11\n    \n    \n      10\n      0.133044\n      0.052796\n      0.868353\n      0.767336\n      0.953055\n      00:11\n    \n    \n      11\n      0.115005\n      0.046287\n      0.871848\n      0.772811\n      0.964127\n      00:11\n    \n    \n      12\n      0.099967\n      0.040762\n      0.883804\n      0.791800\n      0.971401\n      00:11\n    \n    \n      13\n      0.087097\n      0.037157\n      0.889235\n      0.800560\n      0.976527\n      00:11\n    \n    \n      14\n      0.076725\n      0.033152\n      0.899692\n      0.817673\n      0.979948\n      00:12\n    \n    \n      15\n      0.068125\n      0.031111\n      0.902548\n      0.822404\n      0.982294\n      00:11\n    \n    \n      16\n      0.060929\n      0.027760\n      0.910408\n      0.835549\n      0.984009\n      00:11\n    \n    \n      17\n      0.054637\n      0.027974\n      0.894529\n      0.809184\n      0.983233\n      00:11\n    \n    \n      18\n      0.049165\n      0.024334\n      0.911780\n      0.837864\n      0.986022\n      00:11\n    \n    \n      19\n      0.044277\n      0.024281\n      0.905008\n      0.826498\n      0.983716\n      00:11\n    \n    \n      20\n      0.040233\n      0.022132\n      0.913338\n      0.840498\n      0.986142\n      00:11\n    \n    \n      21\n      0.036737\n      0.021514\n      0.913032\n      0.839981\n      0.982759\n      00:12\n    \n    \n      22\n      0.033632\n      0.020441\n      0.919236\n      0.850543\n      0.988929\n      00:11\n    \n    \n      23\n      0.031428\n      0.021564\n      0.900877\n      0.819632\n      0.976978\n      00:11\n    \n    \n      24\n      0.029265\n      0.018416\n      0.922131\n      0.855513\n      0.990599\n      00:11\n    \n    \n      25\n      0.027243\n      0.017509\n      0.924026\n      0.858781\n      0.990082\n      00:11\n    \n    \n      26\n      0.025496\n      0.018210\n      0.916627\n      0.846087\n      0.983598\n      00:11\n    \n    \n      27\n      0.024097\n      0.017901\n      0.917531\n      0.847628\n      0.987033\n      00:11\n    \n    \n      28\n      0.022979\n      0.016894\n      0.920493\n      0.852697\n      0.986602\n      00:11\n    \n    \n      29\n      0.021918\n      0.018380\n      0.916335\n      0.845588\n      0.989573\n      00:11\n    \n    \n      30\n      0.020899\n      0.016375\n      0.920465\n      0.852650\n      0.988758\n      00:11\n    \n    \n      31\n      0.019700\n      0.014662\n      0.929562\n      0.868394\n      0.988694\n      00:11\n    \n    \n      32\n      0.018992\n      0.018643\n      0.906843\n      0.829563\n      0.989722\n      00:11\n    \n    \n      33\n      0.018747\n      0.016715\n      0.918087\n      0.848578\n      0.986600\n      00:11\n    \n    \n      34\n      0.018041\n      0.015872\n      0.918797\n      0.849792\n      0.982712\n      00:11\n    \n    \n      35\n      0.017152\n      0.015625\n      0.922364\n      0.855914\n      0.979253\n      00:11\n    \n    \n      36\n      0.016359\n      0.014718\n      0.925542\n      0.861403\n      0.977279\n      00:11\n    \n    \n      37\n      0.015855\n      0.012931\n      0.932580\n      0.873676\n      0.987032\n      00:11\n    \n    \n      38\n      0.015509\n      0.012442\n      0.932764\n      0.873999\n      0.990109\n      00:11\n    \n    \n      39\n      0.015077\n      0.011627\n      0.936922\n      0.881329\n      0.989619\n      00:11\n    \n    \n      40\n      0.014742\n      0.011964\n      0.934999\n      0.877933\n      0.987285\n      00:11\n    \n    \n      41\n      0.014460\n      0.011784\n      0.937937\n      0.883128\n      0.990435\n      00:11\n    \n    \n      42\n      0.014327\n      0.013164\n      0.928118\n      0.865877\n      0.985723\n      00:11\n    \n    \n      43\n      0.014337\n      0.012003\n      0.934332\n      0.876757\n      0.987117\n      00:11\n    \n    \n      44\n      0.014002\n      0.012138\n      0.935304\n      0.878470\n      0.988768\n      00:11\n    \n    \n      45\n      0.013552\n      0.011391\n      0.936339\n      0.880298\n      0.987371\n      00:11\n    \n    \n      46\n      0.013122\n      0.012408\n      0.929143\n      0.867663\n      0.983431\n      00:11\n    \n    \n      47\n      0.012708\n      0.011765\n      0.932473\n      0.873489\n      0.990839\n      00:12\n    \n    \n      48\n      0.012281\n      0.010892\n      0.938805\n      0.884668\n      0.987427\n      00:11\n    \n    \n      49\n      0.011792\n      0.010427\n      0.941303\n      0.889115\n      0.986761\n      00:11\n    \n    \n      50\n      0.011579\n      0.011140\n      0.938671\n      0.884429\n      0.982990\n      00:11\n    \n    \n      51\n      0.011452\n      0.010431\n      0.945001\n      0.895737\n      0.985848\n      00:11\n    \n    \n      52\n      0.011334\n      0.010210\n      0.943139\n      0.892396\n      0.987449\n      00:11\n    \n    \n      53\n      0.011300\n      0.011088\n      0.937227\n      0.881870\n      0.987982\n      00:11\n    \n    \n      54\n      0.011329\n      0.010670\n      0.937806\n      0.882895\n      0.988177\n      00:11\n    \n    \n      55\n      0.011342\n      0.010610\n      0.937321\n      0.882035\n      0.989792\n      00:11\n    \n    \n      56\n      0.011146\n      0.010468\n      0.938884\n      0.884808\n      0.988132\n      00:11\n    \n    \n      57\n      0.011024\n      0.010291\n      0.940346\n      0.887408\n      0.988785\n      00:11\n    \n    \n      58\n      0.010618\n      0.010369\n      0.939683\n      0.886228\n      0.988485\n      00:11\n    \n    \n      59\n      0.010563\n      0.010375\n      0.939610\n      0.886099\n      0.988654\n      00:11\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 0.6031481027603149.\nBetter model found at epoch 1 with valid_loss value: 0.516569972038269.\nBetter model found at epoch 2 with valid_loss value: 0.3792479634284973.\nBetter model found at epoch 3 with valid_loss value: 0.302621066570282.\nBetter model found at epoch 4 with valid_loss value: 0.20762743055820465.\nBetter model found at epoch 5 with valid_loss value: 0.14218230545520782.\nBetter model found at epoch 6 with valid_loss value: 0.10715709626674652.\nBetter model found at epoch 7 with valid_loss value: 0.0884983241558075.\nBetter model found at epoch 8 with valid_loss value: 0.07099024951457977.\nBetter model found at epoch 9 with valid_loss value: 0.06164400279521942.\nBetter model found at epoch 10 with valid_loss value: 0.0527963824570179.\nBetter model found at epoch 11 with valid_loss value: 0.046286918222904205.\nBetter model found at epoch 12 with valid_loss value: 0.04076218977570534.\nBetter model found at epoch 13 with valid_loss value: 0.037156544625759125.\nBetter model found at epoch 14 with valid_loss value: 0.0331517718732357.\nBetter model found at epoch 15 with valid_loss value: 0.031111164018511772.\nBetter model found at epoch 16 with valid_loss value: 0.027760174125432968.\nBetter model found at epoch 18 with valid_loss value: 0.024333978071808815.\nBetter model found at epoch 19 with valid_loss value: 0.024281086400151253.\nBetter model found at epoch 20 with valid_loss value: 0.022132011130452156.\nBetter model found at epoch 21 with valid_loss value: 0.021513625979423523.\nBetter model found at epoch 22 with valid_loss value: 0.020441483706235886.\nBetter model found at epoch 24 with valid_loss value: 0.018416451290249825.\nBetter model found at epoch 25 with valid_loss value: 0.017508765682578087.\nBetter model found at epoch 28 with valid_loss value: 0.016893696039915085.\nBetter model found at epoch 30 with valid_loss value: 0.016375476494431496.\nBetter model found at epoch 31 with valid_loss value: 0.0146623644977808.\nBetter model found at epoch 37 with valid_loss value: 0.012930507771670818.\nBetter model found at epoch 38 with valid_loss value: 0.01244184747338295.\nBetter model found at epoch 39 with valid_loss value: 0.01162666454911232.\nBetter model found at epoch 45 with valid_loss value: 0.011391145177185535.\nBetter model found at epoch 48 with valid_loss value: 0.010892268270254135.\nBetter model found at epoch 49 with valid_loss value: 0.010426941327750683.\nBetter model found at epoch 52 with valid_loss value: 0.010209585539996624.\n\n\n\nlearn.load(MODEL_NAME)\n\nSaved filed doesn't contain an optimizer state.\n\n\n<fastai.learner.Learner at 0x7f2dd019eac0>\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a dataloader from the testset\ntest_dl = dls.test_dl(test_df, with_labels=True)\ndice_func = monai.metrics.DiceMetric(include_background=False, reduction=\"mean\")\n\n\n# This function steps through the different thresholds and returns the best one\ndef get_best_threshold(learn, dl, metric_func, n_steps=17):\n    \"\"\"\n    Tests `n_steps` different thresholds.\n    Return the best threshold and the corresonding score.\n    \"\"\"\n    thresholds = torch.linspace(0.1, 0.9, n_steps)\n    results = []\n\n    res = learn.get_preds(dl=dl, with_input=False, with_targs=True, act=partial(F.softmax, dim=1))\n    \n    for t in thresholds:\n        metric_func((res[0][:,1]>t).unsqueeze(1), res[-1].unsqueeze(1))\n        metric = metric_func.aggregate().item()\n        metric_func.reset()\n        results.append((round(t.detach().cpu().item(), ndigits=3), metric))\n\n    return sorted(results, key=lambda tpl: tpl[1], reverse=True)[0]\n\n\nbest_threshold, _ = get_best_threshold(learn, dls.valid, dice_func)\nbest_threshold\n\n\n\n\n\n\n\n\n0.6\n\n\nAnd now we test the model on the training, validation and test set with the best threshold.\n\ndef test_model(learn, dl, metric_func, threshold=0.5):\n    res = learn.get_preds(dl=dl, with_input=False, with_targs=True, act=partial(F.softmax, dim=1))\n    metric_func((res[0][:,1]>threshold).unsqueeze(1), res[-1].unsqueeze(1))\n    metric = metric_func.aggregate().item()\n    metric_func.reset()\n    return metric\n    \ntrain_dice = test_model(learn, dls.train, dice_func, threshold=best_threshold)\nvalid_dice = test_model(learn, dls.valid, dice_func, threshold=best_threshold)\ntest_dice  = test_model(learn, test_dl,   dice_func, threshold=best_threshold)\n\nfor s, d in zip((\"Training Dice:\", \"Valid Dice\", \"Test Dice\"), (train_dice, valid_dice, test_dice)):\n    print(s, d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Dice: 0.9252251982688904\nValid Dice 0.9271063208580017\nTest Dice 0.9414731860160828\n\n\n\n# Save and export the model\nBEST_MODEL_NAME = f\"unetpp_b4_th{int(best_threshold*100)}_d{str(test_dice)[2:6]}\"\nlearn.save(BEST_MODEL_NAME)\nlearn.export(BEST_MODEL_NAME+\".pkl\")\n\nA live version of this model is deployed on a huggingface space."
  },
  {
    "objectID": "posts/hpa/protein_atlas_scrape.html",
    "href": "posts/hpa/protein_atlas_scrape.html",
    "title": "Scraping Images from the Human Protein Atlas",
    "section": "",
    "text": "We will be scraping histologic tissue slices from the Human Protein Atlas, which they explicitly allow here.\nThey provide datasets of available images and respective metadata. I am only interested in largeintestine, kidney, lung, spleen, prostate, preferably without stains.\nHere is an (cropped) example:\n\n\nimport numpy as np\nimport pandas as pd\nimport shutil\nfrom bs4 import BeautifulSoup as BS\nimport time\nfrom tqdm import tqdm\nimport requests\nfrom pathlib import Path\nfrom PIL import Image\n\n\ndf = pd.read_csv(\"https://www.proteinatlas.org/download/normal_tissue.tsv.zip\", delimiter=\"\\t\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gene\n      Gene name\n      Tissue\n      Cell type\n      Level\n      Reliability\n    \n  \n  \n    \n      0\n      ENSG00000000003\n      TSPAN6\n      adipose tissue\n      adipocytes\n      Not detected\n      Approved\n    \n    \n      1\n      ENSG00000000003\n      TSPAN6\n      adrenal gland\n      glandular cells\n      Not detected\n      Approved\n    \n    \n      2\n      ENSG00000000003\n      TSPAN6\n      appendix\n      glandular cells\n      Medium\n      Approved\n    \n    \n      3\n      ENSG00000000003\n      TSPAN6\n      appendix\n      lymphoid tissue\n      Not detected\n      Approved\n    \n    \n      4\n      ENSG00000000003\n      TSPAN6\n      bone marrow\n      hematopoietic cells\n      Not detected\n      Approved\n    \n  \n\n\n\n\nLet’s see which tissues they have images of.\n\ndf.Tissue.unique()\n\narray(['adipose tissue', 'adrenal gland', 'appendix', 'bone marrow',\n       'breast', 'bronchus', 'caudate', 'cerebellum', 'cerebral cortex',\n       'cervix', 'colon', 'duodenum', 'endometrium 1', 'endometrium 2',\n       'epididymis', 'esophagus', 'fallopian tube', 'gallbladder',\n       'heart muscle', 'hippocampus', 'kidney', 'liver', 'lung',\n       'lymph node', 'nasopharynx', 'oral mucosa', 'ovary', 'pancreas',\n       'parathyroid gland', 'placenta', 'prostate', 'rectum',\n       'salivary gland', 'seminal vesicle', 'skeletal muscle', 'skin 1',\n       'skin 2', 'small intestine', 'smooth muscle', 'soft tissue 1',\n       'soft tissue 2', 'spleen', 'stomach 1', 'stomach 2', 'testis',\n       'thyroid gland', 'tonsil', 'urinary bladder', 'vagina', nan,\n       'hypothalamus', 'endometrium', 'hair', 'retina',\n       'lactating breast', 'skin', 'thymus', 'cartilage', 'eye',\n       'pituitary gland', 'choroid plexus', 'dorsal raphe',\n       'substantia nigra', 'sole of foot'], dtype=object)\n\n\nEvery tissue I am interest in is present! (Colon = large Intestine) Let’s get rid of everything else.\n\ntissues = set(\"colon kidney lung prostate spleen\".split())\nmask = df.Tissue.isin(tissues)\ndf = df[mask]\ndf.Tissue.unique()\n\narray(['colon', 'kidney', 'lung', 'prostate', 'spleen'], dtype=object)\n\n\nAll images have been stained with some antibody, but we actually don’t want stained images. That’s why we’re only keeping those images in which no antigen has been detected (meaning no visible stain is present in these images).\n\ndf = df[df.Level == \"Not detected\"]\ndf.Level.unique()\n\narray(['Not detected'], dtype=object)\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Gene\n      Gene name\n      Tissue\n      Cell type\n      Level\n      Reliability\n    \n  \n  \n    \n      20\n      ENSG00000000003\n      TSPAN6\n      colon\n      endothelial cells\n      Not detected\n      Approved\n    \n    \n      22\n      ENSG00000000003\n      TSPAN6\n      colon\n      peripheral nerve/ganglion\n      Not detected\n      Approved\n    \n    \n      35\n      ENSG00000000003\n      TSPAN6\n      kidney\n      cells in glomeruli\n      Not detected\n      Approved\n    \n    \n      40\n      ENSG00000000003\n      TSPAN6\n      lung\n      macrophages\n      Not detected\n      Approved\n    \n    \n      67\n      ENSG00000000003\n      TSPAN6\n      spleen\n      cells in red pulp\n      Not detected\n      Approved\n    \n  \n\n\n\n\nCell types and reliability are not relevant in this usecase, so let’s just get rid of those columns. Also, the actual name of the gene is redundant.\n\ndel df[\"Cell type\"], df[\"Reliability\"], df[\"Level\"], df[\"Gene name\"]\ndf\n\n\n\n\n\n  \n    \n      \n      Gene\n      Tissue\n    \n  \n  \n    \n      20\n      ENSG00000000003\n      colon\n    \n    \n      22\n      ENSG00000000003\n      colon\n    \n    \n      35\n      ENSG00000000003\n      kidney\n    \n    \n      40\n      ENSG00000000003\n      lung\n    \n    \n      67\n      ENSG00000000003\n      spleen\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1194410\n      ENSG00000288602\n      kidney\n    \n    \n      1194414\n      ENSG00000288602\n      lung\n    \n    \n      1194415\n      ENSG00000288602\n      lung\n    \n    \n      1194416\n      ENSG00000288602\n      lung\n    \n    \n      1194457\n      ENSG00000288602\n      spleen\n    \n  \n\n67054 rows × 2 columns\n\n\n\nSome genes appear multiple times per organ. Let’s remove those duplicates.\n\ndf = df.drop_duplicates(subset=[\"Gene\", \"Tissue\"])\ndf\n\n\n\n\n\n  \n    \n      \n      Gene\n      Tissue\n    \n  \n  \n    \n      20\n      ENSG00000000003\n      colon\n    \n    \n      35\n      ENSG00000000003\n      kidney\n    \n    \n      40\n      ENSG00000000003\n      lung\n    \n    \n      67\n      ENSG00000000003\n      spleen\n    \n    \n      196\n      ENSG00000000457\n      lung\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1194296\n      ENSG00000288558\n      lung\n    \n    \n      1194388\n      ENSG00000288602\n      colon\n    \n    \n      1194410\n      ENSG00000288602\n      kidney\n    \n    \n      1194414\n      ENSG00000288602\n      lung\n    \n    \n      1194457\n      ENSG00000288602\n      spleen\n    \n  \n\n37915 rows × 2 columns\n\n\n\nNow, how many genes per organ do we get?\n\ndf.groupby(\"Tissue\").count()\n\n\n\n\n\n  \n    \n      \n      Gene\n    \n    \n      Tissue\n      \n    \n  \n  \n    \n      colon\n      7338\n    \n    \n      kidney\n      7732\n    \n    \n      lung\n      8747\n    \n    \n      prostate\n      5174\n    \n    \n      spleen\n      8924\n    \n  \n\n\n\n\nTo many to just download all at once, so we will randomly select 25 of each tissue. Since many genes have been more than one image, we will and up which 2-3 times as many images.\n\nrand_indices = []\nfor t in df.Tissue.unique():\n    rand_idx = list(np.random.choice(df[df.Tissue==t].index, size=25))\n    rand_indices = [*rand_indices, *rand_idx]\ndf = df.loc[rand_indices]\ndf.groupby(\"Tissue\").count()\n\n\n\n\n\n  \n    \n      \n      Gene\n    \n    \n      Tissue\n      \n    \n  \n  \n    \n      colon\n      25\n    \n    \n      kidney\n      25\n    \n    \n      lung\n      25\n    \n    \n      prostate\n      25\n    \n    \n      spleen\n      25\n    \n  \n\n\n\n\nNow, we need to build the URL’s of those images. Most genes have multiple images so we are going to expand our dataset with links to every image of every gene.\n\ndef build_url(embl, tissue): return f\"https://www.proteinatlas.org/{embl}/tissue/{tissue}\"\ndef extract_image_urls(htext):\n    soup = BS(htext)\n    image_urls = soup.body.findAll(\"img\")\n    image_urls = [il.get_attribute_list(\"src\")[0] for il in image_urls]\n    image_urls = [il for il in image_urls if \"proteinatlas\" in il]\n    image_urls = [\"https:\"+im_url for im_url in image_urls]\n    return image_urls\n\n\nurls = []\nfor gene, tissue in tqdm(list(zip(df.Gene.values, df.Tissue.values))):\n    url = build_url(gene, tissue)\n    r = requests.get(url)\n    if r.status_code != 200:\n        print(f\"Url: {url} didn't work\")\n        urls.append(None)\n        continue\n    urls.append(\";\".join(extract_image_urls(r.text)))\n    time.sleep(0.1)\ndf[\"urls\"] = urls\n\n100%|██████████| 125/125 [01:39<00:00,  1.25it/s]\n\n\nWe save the data, so that we don’t have to run this code twice.\n\ndf.to_csv(\"protein_atlas_scrape.csv\", index=False)\n\nAnd finally, we download the images.\n\ndef download_image(image_url, filename):\n    try:\n        r = requests.get(image_url, stream=True)\n        if r.status_code == 200:\n            r.raw.decode_content = True\n            with open(filename,'wb') as f: shutil.copyfileobj(r.raw, f)\n        else: return\n    except: return\n\n\nDOWNLOAD_DIR = Path(\"images\")\nDOWNLOAD_DIR.mkdir(exist_ok=True)\nfor _,row in tqdm(df.iterrows(), total=len(df)):\n    for i, url in enumerate(row.urls.split(\";\")):\n        image_name = DOWNLOAD_DIR/f\"{row.Gene}_{row.Tissue}_{i}.jpg\"\n        download_image(url, image_name)\n        time.sleep(0.1)\n\n100%|██████████| 125/125 [03:17<00:00,  1.58s/it]\n\n\n\nlen(list(Path(\"images\").iterdir()))\n\n506\n\n\nNow we have 506 images! Let’s look at a random one.\n\nimage_file = np.random.choice(list(Path(\"images\").iterdir()))\nImage.open(image_file)\n\n\n\n\nRandom Image from our new new dataset.\n\n\n\n\nAnd there we go! We now have histologic images we can use for anything we like."
  },
  {
    "objectID": "blog_overview.html",
    "href": "blog_overview.html",
    "title": "Blog",
    "section": "",
    "text": "Fine-tuning Custom Language Models with Hugging Face and Unsloth\n\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\nFavian Hatje\n\n\n\n\n\n\n  \n\n\n\n\nGlomerulus Segmentation\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\nFavian Hatje\n\n\n\n\n\n\n  \n\n\n\n\nScraping Images from the Human Protein Atlas\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2022\n\n\nFavian Hatje\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "old_index.html",
    "href": "old_index.html",
    "title": "Welcome",
    "section": "",
    "text": "Favian Hatje is a radiologist with a passion for Science, especially data science and artificial intelligence, currently working for Ihre-Radiologen in Berlin. He studied Medicine in Hamburg where he also worked fulltime in a physiology lab for his medical doctorate and, when not busy reading CT or MRI studies, spends most of his time competing on Kaggle.\n\n\n\n\n\n\nMedicine\nArtificial Intelligence\n(Data) Science\n\n\n\n\n\nMedical Studies in Hamburg, Germany 2012 - 2019\nMedical Doctorate 2017 - 2022\n\n\n\n\n\nHatje FA, Wedekind U, Sachs W, Loreth D, Reichelt J, Demir F, Kosub C, Heintz L, Tomas NM, Huber TB, Skuza S, Sachs M, Zielinski S, Rinschen MM, Meyer-Schwesinger C. Tripartite Separation of Glomerular Cell Types and Proteomes from Reporter-Free Mice. J Am Soc Nephrol. 2021 Sep;32(9):2175-2193. doi: 10.1681/ASN.2020091346. Epub 2021 Jun 1. PMID: 34074698; PMCID: PMC8729851."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am a radiologist with a passion for science and technology. I have many interests, but I am particularly drawn to the field of artificial intelligence. I am based in Berlin, Germany, where I work for Ihre-Radiologen.\nMy journey began in Hamburg, where I studied medicine, complemented by one year of full-time work in a physiology lab. There, my colleagues and I developed a method for extracting large quantities of glomerular cells from mice, which resulted in a publication in the Journal of the American Society of Nephrology (JASN), a renowned journal in the field of nephrology.\nI then made my way to Berlin, where I began my career as a radiologist. By day, I am busy reading CT and MRI studies, at night I enjoy exploring the limitless possibilities and incredible developments of artificial intelligence. Through continued self-study through courses like fast.ai and Deep Learning for Medicine on Coursera, I have deepened my understanding of this exciting field, and I regularly participate in Kaggle competitions to hone my skills.\nIf you wish to connect, feel free to reach out on Twitter or via email at favian.hatje@outlook.com. Otherwise I invite you to have a look at my blog, where I share notebooks documenting what I do and learn.\n\nEducation:\n\nMedical Studies in Hamburg, Germany 2012 - 2019\nMedical Doctorate 2017 - 2024\n\n\n\nPublications:\n\nHatje FA, Wedekind U, Sachs W, Loreth D, Reichelt J, Demir F, Kosub C, Heintz L, Tomas NM, Huber TB, Skuza S, Sachs M, Zielinski S, Rinschen MM, Meyer-Schwesinger C. Tripartite Separation of Glomerular Cell Types and Proteomes from Reporter-Free Mice. J Am Soc Nephrol. 2021 Sep;32(9):2175-2193. doi: 10.1681/ASN.2020091346. Epub 2021 Jun 1. PMID: 34074698; PMCID: PMC8729851.\nSachs W, Blume L, Loreth D, Schebsdat L, Hatje F, Koehler S, Wedekind U, Sachs M, Zieliniski S, Brand J, Conze C, Florea BI, Heppner F, Krüger E, Rinschen MM, Kretz O, Thünauer R, Meyer-Schwesinger C. The proteasome modulates endocytosis specifically in glomerular cells to promote kidney filtration. Nat Commun. 2024 Mar 1;15(1):1897. doi: 10.1038/s41467-024-46273-0. PMID: 38429282; PMCID: PMC10907641."
  },
  {
    "objectID": "posts/hf_unsloth/huggingface_unsloth.html",
    "href": "posts/hf_unsloth/huggingface_unsloth.html",
    "title": "Fine-tuning Custom Language Models with Hugging Face and Unsloth",
    "section": "",
    "text": "Fine-Tuning Large Language Models with Custom Data Using Hugging Face and Unsloth on a Single GPU\nThis image was generated using DALL-E and the following prompt: A sloth running through a jungle, carrying a huggingface emoji in its arms. The emoji has hearts as eyes. Motionblur in the background. Flaming foot steps. 3D rendering. 16:9 aspect ratio."
  },
  {
    "objectID": "posts/hf_unsloth/huggingface_unsloth.html#install",
    "href": "posts/hf_unsloth/huggingface_unsloth.html#install",
    "title": "Fine-tuning Custom Language Models with Hugging Face and Unsloth",
    "section": "Install",
    "text": "Install\nUse a env. Start by installing unsloth, following their instructions."
  },
  {
    "objectID": "posts/hf_unsloth/huggingface_unsloth.html#dataset-creation",
    "href": "posts/hf_unsloth/huggingface_unsloth.html#dataset-creation",
    "title": "Fine-tuning Custom Language Models with Hugging Face and Unsloth",
    "section": "Dataset Creation",
    "text": "Dataset Creation\nConvert your raw conversation or text data into a list of lists of dictionaries, as described in the Unsloth wiki:\n[\n    [{\"from\": \"human\", \"value\": \"Hi there!\"},\n     {\"from\": \"gpt\", \"value\": \"Hi how can I help?\"},\n     {\"from\": \"human\", \"value\": \"What is 2+2?\"}],\n    [{\"from\": \"human\", \"value\": \"What's your name?\"},\n     {\"from\": \"gpt\", \"value\": \"I'm Daniel!\"},\n     {\"from\": \"human\", \"value\": \"Ok! Nice!\"},\n     {\"from\": \"gpt\", \"value\": \"What can I do for you?\"},\n     {\"from\": \"human\", \"value\": \"Oh nothing :)\"},],\n]\nNext, determine the format required for parsing the data before tokenization. Begin by loading the tokenizer, alongside the model. For this tutorial, we will use the google/gemma-1.1-2b-it model. Despite its seemingly large size, it is relatively small with only 2.51 billion parameters.\n\nfrom unsloth import FastLanguageModel\n\nbase_model = \"google/gemma-1.1-2b-it\"\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = base_model,\n    load_in_4bit=True, # Load the model in 4-bit mode\n)\n\n==((====))==  Unsloth: Fast Gemma patching release 2024.4\n   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.6. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n\n\n\n\n\n\ntokenizer.chat_template\n\n\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n\n\nThe chat template utilizes a Jinja template. Due to Jinja’s lenient handling of whitespaces and new lines, it’s crucial to remove any unnecessary whitespaces and new lines to ensure clarity.\nBelow is the template presented in a more readable format:\n{{ bos_token }}\n{% if messages[0]['role'] == 'system' %}\n    {{ raise_exception('System role not supported') }}\n{% endif %}\n{% for message in messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n    {{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}\n{% endfor %}\n{% if add_generation_prompt %}\n    {{'<start_of_turn>model\\n'}}\n{% endif %}\nNote that each message is represented as a dictionary containing a from and value field. In the data, the from values are labeled as gpt and human, whereas in the chat template, they are identified as assistant and user. To align these, it is simpler to adjust the data. However, if required, roles can be modified in the tokenizer using the mapping argument:\nmapping = {\n    \"role\" : \"from\", \n    \"content\" : \"value\", \n    \"user\" : \"human\", \n    \"assistant\" : \"gpt\"\n}\n\nfrom unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = (\n        tokenizer.chat_template, # we are not changing anything here,\n        tokenizer.eos_token),    # just passing in the default values\n    mapping = {\n        \"role\" : \"from\",     # Change 'role' to ‘from'\n        \"content\" : \"value\", # Change 'content' to value'\n        \"user\" : \"human\",    # Default and not relevant here\n        \"assistant\" : \"gpt\"  # Default and not relevant here\n        },\n    map_eos_token = False,\n)\n\n\ndata = [\n    # Conversation 1\n    [{\"from\": \"human\", \"value\": \"Hi there!\"},\n     {\"from\": \"gpt\", \"value\": \"Hi how can I help?\"},\n     {\"from\": \"human\", \"value\": \"What is 2+2?\"}],\n    # Conversation 2\n    [{\"from\": \"human\", \"value\": \"What's your name?\"},\n     {\"from\": \"gpt\", \"value\": \"I'm Daniel!\"},\n     {\"from\": \"human\", \"value\": \"Ok! Nice!\"},\n     {\"from\": \"gpt\", \"value\": \"What can I do for you?\"},\n     {\"from\": \"human\", \"value\": \"Oh nothing :)\"},],\n]\n\n\ntokenizer.apply_chat_template(data[0], tokenize=False)\n\n'<bos><start_of_turn>human\\nHi there!<end_of_turn>\\n<start_of_turn>model\\nHi how can I help?<end_of_turn>\\n<start_of_turn>human\\nWhat is 2+2?<end_of_turn>\\n'\n\n\nNow that both the data and tokenizer are set up, you can proceed to create a Hugging Face dataset:\n\nfrom datasets import Dataset\n\n# Hugging Face expects data in the following format\ndata = {\"samples\": data}\ndataset = Dataset.from_dict(data)\n\n# Optionally, create training and testing splits, \n# and push the dataset to the Hugging Face Hub:\n\n# dataset = dataset.train_test_split(test_size=0.1)\n# dataset.push_to_hub(\"new_custom_dataset\")\n\n# To load the dataset from the hub:\n# dataset = load_dataset(\"your_huggingface_name/new_custom_dataset\")"
  },
  {
    "objectID": "posts/hf_unsloth/huggingface_unsloth.html#peft-training",
    "href": "posts/hf_unsloth/huggingface_unsloth.html#peft-training",
    "title": "Fine-tuning Custom Language Models with Hugging Face and Unsloth",
    "section": "PEFT Training",
    "text": "PEFT Training\nTo optimize memory usage and speed during fine-tuning, we will employ a technique known as QLORA. Unsloth manages the quantization and LoRA (Low-Rank Adaptation) parameters for us, streamlining the process.\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Rank of the lora adapters\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n    bias = \"none\",      # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = True,\n    random_state = 3407,\n    max_seq_length = 2048,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)\n\nUnsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2024.4 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n\n\n\nimport torch\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    dataset_text_field = \"preprocessed\",\n    max_seq_length = 2048,\n    tokenizer = tokenizer,\n    args = TrainingArguments(\n        # Adjust the parameters to your needs\n        per_device_train_batch_size = 8,\n        gradient_accumulation_steps = 1,\n        warmup_steps = 1,       \n        max_steps = 5,          \n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,      \n        save_steps = 100,       \n        output_dir = \"new_model_name\",\n        optim = \"adamw_8bit\",\n        report_to=\"tensorboard\",\n        learning_rate = 1e-3,\n    ),\n)\n\n\n\n\n\ntrainer.train()\n\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 2 | Num Epochs = 5\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n\\        /    Total batch size = 8 | Total steps = 5\n \"-____-\"     Number of trainable parameters = 19,611,648\n\n\n\n\n    \n      \n      \n      [5/5 00:00, Epoch 5/5]\n    \n    \n  \n \n      Step\n      Training Loss\n    \n  \n  \n    \n      1\n      16.625000\n    \n    \n      2\n      16.625000\n    \n    \n      3\n      9.937500\n    \n    \n      4\n      7.281200\n    \n    \n      5\n      5.968800\n    \n  \n\n\n\nTrainOutput(global_step=5, training_loss=11.2875, metrics={'train_runtime': 1.4908, 'train_samples_per_second': 26.831, 'train_steps_per_second': 3.354, 'total_flos': 7880446279680.0, 'train_loss': 11.2875, 'epoch': 5.0})\n\n\nOnce the model is trained, you can merge the LoRA adapters into the base model to finalize your newly fine-tuned model.\n\nmodel.save_pretrained_merged(\"new_model\", tokenizer, save_method = \"merged_16bit\",)\n# alternatively we can push it to the huggingface hub\n# model.push_to_hub_merged(\"your hf_name/new_model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 18.43 out of 31.25 RAM for saving.\n\n\n100%|██████████| 18/18 [00:00<00:00, 128.34it/s]\n\n\nUnsloth: Saving tokenizer...\n\n\n\n\n\n Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nDone.\n\n\nAnd there you have it—a large language model fine-tuned on our custom dataset. Both the model and tokenizer can now be loaded using Hugging Face alone. Unsloth also provides an inference solution. Enjoy exploring the capabilities of your fine-tuned model!"
  },
  {
    "objectID": "posts/hf_unsloth/huggingface_unsloth.html#installation",
    "href": "posts/hf_unsloth/huggingface_unsloth.html#installation",
    "title": "Fine-tuning Custom Language Models with Hugging Face and Unsloth",
    "section": "Installation",
    "text": "Installation\nBegin by setting up a dedicated environment. To install Unsloth, follow the instructions provided on their GitHub page."
  },
  {
    "objectID": "posts/hf_unsloth/huggingface_unsloth.html#preprocessing-the-dataset-for-training",
    "href": "posts/hf_unsloth/huggingface_unsloth.html#preprocessing-the-dataset-for-training",
    "title": "Fine-tuning Custom Language Models with Hugging Face and Unsloth",
    "section": "Preprocessing the Dataset for Training",
    "text": "Preprocessing the Dataset for Training\nFinally, preprocess the dataset for training by mapping each entry through a function that applies the chat template using the tokenizer. This step prepares the data without converting it into tokens yet.\n\n# Lastly, we preprocess the dataset for training\ndataset = dataset.map(\n    lambda x: {\n        \"preprocessed\": tokenizer.apply_chat_template(\n            x[\"samples\"], \n            tokenize=False,              # Avoid converting text into tokens at this stage\n            add_generation_prompt=False, # Required for setting up inference\n            add_special_tokens=False     # May be necessary depending on model specifics\n        )\n    }\n)\n\n\n\n\nNow our dataset features two fields: samples, which contains the raw conversations, and preprocessed, where the chat template has been applied. Additionally, special tokens have been added to the preprocessed data:\n\ndataset[\"samples\"][0]\n\n[{'from': 'human', 'value': 'Hi there!'},\n {'from': 'gpt', 'value': 'Hi how can I help?'},\n {'from': 'human', 'value': 'What is 2+2?'}]\n\n\n\ndataset[\"preprocessed\"][0]\n\n'<bos><start_of_turn>human\\nHi there!<end_of_turn>\\n<start_of_turn>model\\nHi how can I help?<end_of_turn>\\n<start_of_turn>human\\nWhat is 2+2?<end_of_turn>\\n'\n\n\nEnsure that the preprocessed output is formatted precisely as you and the model require. If there is any uncertainty about the format, consult the respective paper or documentation associated with the model to verify the expected data structure and formatting details. This is crucial because any discrepancies in format can lead to a degradation in model performance."
  },
  {
    "objectID": "posts/hf_unsloth/huggingface_unsloth.html#why-unsloth",
    "href": "posts/hf_unsloth/huggingface_unsloth.html#why-unsloth",
    "title": "Fine-tuning Custom Language Models with Hugging Face and Unsloth",
    "section": "Why Unsloth?",
    "text": "Why Unsloth?\nUnsloth is a relatively new library that offers speed and ease of use. It employs quantization and is built on top of Hugging Face, providing support for models like Mistral, Gemma, and Llama. If your preferred model is supported, Unsloth is currently one of the best options available for fine-tuning large language models."
  }
]