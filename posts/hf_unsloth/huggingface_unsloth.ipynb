{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Fine-tuning Custom Language Models with Hugging Face and Unsloth\"\n",
    "author: Favian Hatje\n",
    "date: '04.21.2024'\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "categories: [deep learning, huggingface, unsloth, LLM, finetuning]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning Large Language Models with Custom Data Using Hugging Face and Unsloth on a Single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Unsloth_Huggingface.webp)\n",
    "\n",
    "<span style=\"color:gray;font-size:small;\">\n",
    "This image was generated using DALL-E and the following prompt:</br>\n",
    "_A sloth running through a jungle, carrying a huggingface emoji in its arms. The emoji has hearts as eyes. Motionblur in the background. Flaming foot steps. 3D rendering. 16:9 aspect ratio._\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Unsloth?\n",
    "\n",
    "Unsloth is a relatively new library that offers speed and ease of use. It employs quantization and is built on top of Hugging Face, providing support for models like Mistral, Gemma, and Llama. If your preferred model is supported, Unsloth is currently one of the best options available for fine-tuning large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Begin by setting up a dedicated environment. To install [Unsloth](https://unsloth.ai), follow the instructions provided on their [GitHub page](https://github.com/unslothai/unsloth?tab=readme-ov-file#conda-installation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "Convert your raw conversation or text data into a list of lists of dictionaries, as described in the [Unsloth wiki](https://github.com/unslothai/unsloth/wiki#chat-templates):\n",
    "\n",
    "```\n",
    "[\n",
    "    [{\"from\": \"human\", \"value\": \"Hi there!\"},\n",
    "     {\"from\": \"gpt\", \"value\": \"Hi how can I help?\"},\n",
    "     {\"from\": \"human\", \"value\": \"What is 2+2?\"}],\n",
    "    [{\"from\": \"human\", \"value\": \"What's your name?\"},\n",
    "     {\"from\": \"gpt\", \"value\": \"I'm Daniel!\"},\n",
    "     {\"from\": \"human\", \"value\": \"Ok! Nice!\"},\n",
    "     {\"from\": \"gpt\", \"value\": \"What can I do for you?\"},\n",
    "     {\"from\": \"human\", \"value\": \"Oh nothing :)\"},],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, determine the format required for parsing the data before tokenization. Begin by loading the tokenizer, alongside the model. For this tutorial, we will use the [google/gemma-1.1-2b-it](https://huggingface.co/google/gemma-1.1-2b-it) model. Despite its seemingly large size, it is relatively small with only 2.51 billion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4845115dd3da4e7b84e1a742289ce9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "base_model = \"google/gemma-1.1-2b-it\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model,\n",
    "    load_in_4bit=True, # Load the model in 4-bit mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat template utilizes a Jinja template. Due to Jinja's lenient handling of whitespaces and new lines, it's crucial to remove any unnecessary whitespaces and new lines to ensure clarity.\n",
    "\n",
    "Below is the template presented in a more readable format:\n",
    "\n",
    "```\n",
    "{{ bos_token }}\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "    {{ raise_exception('System role not supported') }}\n",
    "{% endif %}\n",
    "{% for message in messages %}\n",
    "    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
    "        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
    "    {% endif %}\n",
    "    {% if (message['role'] == 'assistant') %}\n",
    "        {% set role = 'model' %}\n",
    "    {% else %}\n",
    "        {% set role = message['role'] %}\n",
    "    {% endif %}\n",
    "    {{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "    {{'<start_of_turn>model\\n'}}\n",
    "{% endif %}\n",
    "```\n",
    "\n",
    "Note that each message is represented as a dictionary containing a `from` and `value` field. In the data, the `from` values are labeled as _gpt_ and _human_, whereas in the chat template, they are identified as _assistant_ and _user_. To align these, it is simpler to adjust the data. However, if required, roles can be modified in the tokenizer using the mapping argument:\n",
    "\n",
    "```\n",
    "mapping = {\n",
    "    \"role\" : \"from\", \n",
    "    \"content\" : \"value\", \n",
    "    \"user\" : \"human\", \n",
    "    \"assistant\" : \"gpt\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = (\n",
    "        tokenizer.chat_template, # we are not changing anything here,\n",
    "        tokenizer.eos_token),    # just passing in the default values\n",
    "    mapping = {\n",
    "        \"role\" : \"from\",     # Change 'role' to ‘from'\n",
    "        \"content\" : \"value\", # Change 'content' to value'\n",
    "        \"user\" : \"human\",    # Default and not relevant here\n",
    "        \"assistant\" : \"gpt\"  # Default and not relevant here\n",
    "        },\n",
    "    map_eos_token = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    # Conversation 1\n",
    "    [{\"from\": \"human\", \"value\": \"Hi there!\"},\n",
    "     {\"from\": \"gpt\", \"value\": \"Hi how can I help?\"},\n",
    "     {\"from\": \"human\", \"value\": \"What is 2+2?\"}],\n",
    "    # Conversation 2\n",
    "    [{\"from\": \"human\", \"value\": \"What's your name?\"},\n",
    "     {\"from\": \"gpt\", \"value\": \"I'm Daniel!\"},\n",
    "     {\"from\": \"human\", \"value\": \"Ok! Nice!\"},\n",
    "     {\"from\": \"gpt\", \"value\": \"What can I do for you?\"},\n",
    "     {\"from\": \"human\", \"value\": \"Oh nothing :)\"},],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>human\\nHi there!<end_of_turn>\\n<start_of_turn>model\\nHi how can I help?<end_of_turn>\\n<start_of_turn>human\\nWhat is 2+2?<end_of_turn>\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(data[0], tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both the data and tokenizer are set up, you can proceed to create a Hugging Face dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Hugging Face expects data in the following format\n",
    "data = {\"samples\": data}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Optionally, create training and testing splits, \n",
    "# and push the dataset to the Hugging Face Hub:\n",
    "\n",
    "# dataset = dataset.train_test_split(test_size=0.1)\n",
    "# dataset.push_to_hub(\"new_custom_dataset\")\n",
    "\n",
    "# To load the dataset from the hub:\n",
    "# dataset = load_dataset(\"your_huggingface_name/new_custom_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset for Training\n",
    "\n",
    "Finally, preprocess the dataset for training by mapping each entry through a function that applies the chat template using the tokenizer. This step prepares the data without converting it into tokens yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aec506249de431bbe355958f8ce970b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lastly, we preprocess the dataset for training\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        \"preprocessed\": tokenizer.apply_chat_template(\n",
    "            x[\"samples\"], \n",
    "            tokenize=False,              # Avoid converting text into tokens at this stage\n",
    "            add_generation_prompt=False, # Required for setting up inference\n",
    "            add_special_tokens=False     # May be necessary depending on model specifics\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset features two fields: `samples`, which contains the raw conversations, and `preprocessed`, where the chat template has been applied. Additionally, special tokens have been added to the preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'human', 'value': 'Hi there!'},\n",
       " {'from': 'gpt', 'value': 'Hi how can I help?'},\n",
       " {'from': 'human', 'value': 'What is 2+2?'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"samples\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>human\\nHi there!<end_of_turn>\\n<start_of_turn>model\\nHi how can I help?<end_of_turn>\\n<start_of_turn>human\\nWhat is 2+2?<end_of_turn>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"preprocessed\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the preprocessed output is formatted precisely as you and the model require. If there is any uncertainty about the format, consult the respective paper or documentation associated with the model to verify the expected data structure and formatting details. This is crucial because any discrepancies in format can lead to a degradation in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT Training\n",
    "\n",
    "To optimize memory usage and speed during fine-tuning, we will employ a technique known as [QLORA](https://arxiv.org/abs/2305.14314). Unsloth manages the quantization and LoRA (Low-Rank Adaptation) parameters for us, streamlining the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2024.4 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Rank of the lora adapters\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    max_seq_length = 2048,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e6a978fc4e40d3844bbfbba425b605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"preprocessed\",\n",
    "    max_seq_length = 2048,\n",
    "    tokenizer = tokenizer,\n",
    "    args = TrainingArguments(\n",
    "        # Adjust the parameters to your needs\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 1,       \n",
    "        max_steps = 5,          \n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,      \n",
    "        save_steps = 100,       \n",
    "        output_dir = \"new_model_name\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        report_to=\"tensorboard\",\n",
    "        learning_rate = 1e-3,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 8 | Total steps = 5\n",
      " \"-____-\"     Number of trainable parameters = 19,611,648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>16.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.968800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=11.2875, metrics={'train_runtime': 1.4908, 'train_samples_per_second': 26.831, 'train_steps_per_second': 3.354, 'total_flos': 7880446279680.0, 'train_loss': 11.2875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, you can merge the LoRA adapters into the base model to finalize your newly fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 18.43 out of 31.25 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 128.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"new_model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "# alternatively we can push it to the huggingface hub\n",
    "# model.push_to_hub_merged(\"your hf_name/new_model\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it—a large language model fine-tuned on our custom dataset. Both the model and tokenizer can now be loaded using Hugging Face alone. Unsloth also provides an inference solution. Enjoy exploring the capabilities of your fine-tuned model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
